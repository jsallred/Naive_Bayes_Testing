{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "933721ea-1380-408f-b886-7dfb7b2d2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score  # Import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eb883dd-2e5a-44d5-9a90-1b176a3b9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess the data\n",
    "df = pd.read_csv('BABE_scraped.csv')\n",
    "df['content'] = df['content'].str.lower()  # Convert text to lowercase\n",
    "\n",
    "df.dropna(subset=['content'], inplace=True) # Drop rows with missing values in the 'content' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52e9cce9-fde3-4b3a-8c0e-34e75f42c5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Feature extraction...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Feature extraction\n",
    "print(\"Step 2: Feature extraction...\")\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['content'])\n",
    "y = df['type_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bec8731-fbc4-4d84-9180-3231d771f60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Splitting data into training, validation, and testing sets...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Split data into training, validation, and testing sets\n",
    "print(\"Step 3: Splitting data into training, validation, and testing sets...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc1e387a-f072-4415-a6ca-8f56cd7b986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Training the Naive Bayes classifier...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Train the Naive Bayes classifier\n",
    "print(\"Step 4: Training the Naive Bayes classifier...\")\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb2c8464-9fee-418a-bca0-ad3dce23cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Evaluating the model on the validation set...\n",
      "Validation Accuracy: 0.6790123456790124\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the model on the validation set\n",
    "print(\"Step 5: Evaluating the model on the validation set...\")\n",
    "y_val_pred = clf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b409234b-0ca8-4c6e-934b-8706a278c36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Evaluating the model on the test set...\n",
      "Test Accuracy: 0.7484662576687117\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate the model on the test set\n",
    "print(\"Step 6: Evaluating the model on the test set...\")\n",
    "y_test_pred = clf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6514bc1c-951d-499e-b220-8c8789e0340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.92      0.82        90\n",
      "           1       1.00      0.00      0.00        11\n",
      "           2       0.78      0.63      0.70        62\n",
      "\n",
      "    accuracy                           0.75       163\n",
      "   macro avg       0.84      0.52      0.50       163\n",
      "weighted avg       0.77      0.75      0.72       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Print classification report for test set\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17d9fe31-69a5-4d8e-b9f1-a8a345640f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.69846154 0.70769231 0.63692308 0.70153846 0.68      ]\n",
      "Mean CV Accuracy: 0.684923076923077\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Add article length as a feature\n",
    "df['article_length'] = df['content'].apply(len)\n",
    "\n",
    "# Combine text features with custom features\n",
    "X_custom = hstack([X, df['article_length'].values.reshape(-1, 1)])\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(clf, X_custom, y, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96fd0b-0e6e-445c-a1ce-0a820dae1325",
   "metadata": {},
   "source": [
    "**Data Loading and Preprocessing**\n",
    "- The dataset (BABE_scraped.csv) contains news articles where each article is labeled with a political bias class (0 for left, 1 for center, and 2 for right).\n",
    "- Text preprocessing involves converting the text to lowercase and removing rows with missing content.\n",
    "\n",
    "**Feature Extraction**\n",
    "- Text data is transformed into numerical feature vectors using TF-IDF (Term Frequency-Inverse Document Frequency) representation. This process captures the importance of words in the documents relative to the entire corpus.\n",
    "- Stop words are removed, and only the top 1000 most frequent words are considered.\n",
    "\n",
    "**Data Splitting**\n",
    "- The dataset is split into training, validation, and test sets with a ratio of 80:10:10 respectively. This ensures that the model is trained on a majority of the data while still having separate sets for validation and final evaluation.\n",
    "\n",
    "**Model Training**\n",
    "- The Multinomial Naive Bayes classifier is trained on the training data. Naive Bayes classifiers are commonly used for text classification tasks due to their simplicity and effectiveness with high-dimensional data like text.\n",
    "\n",
    "**Model Evaluation on Validation Set**\n",
    "- The trained model's performance is evaluated on the validation set. Accuracy, precision, recall, and F1-score are computed to assess how well the model classifies the political biases in the validation data.\n",
    "\n",
    "**Model Evaluation on Test Set**\n",
    "- The model's performance is further evaluated on the test set to ensure its generalization ability. Accuracy metrics are computed to determine how well the model performs on unseen data.\n",
    "\n",
    "**Classification Report**\n",
    "- The classification report provides detailed metrics for each political bias class (left, center, right) including precision, recall, and F1-score. This helps in understanding the model's performance for each class individually.\n",
    "\n",
    "**Custom Feature Addition and Cross-Validation**\n",
    "- The custom feature added to the model is the \"article length.\" This feature represents the length (number of characters, words, or sentences) of each news article in the dataset. By incorporating this additional information into the feature matrix, the model can potentially capture patterns related to the length of articles and how it correlates with their political bias classification.\n",
    "- In the context of this NLP project, adding the article length as a feature serves two primary purposes:\n",
    "    - **Additional Information Incorporation**\n",
    "    - By including the article length as a feature, the model gains additional information beyond just the textual content of the articles. This can help the model better differentiate between articles of different lengths and potentially capture any correlations between article length and political bias.\n",
    "    - Testing for Correlation between Length and Bias:\n",
    "    - The inclusion of article length as a feature allows the model to test whether there is a correlation between the length of news articles and their political bias classification. It enables the model to learn if certain political biases tend to manifest in longer or shorter articles, which could provide insights into how different biases are expressed in media content.\n",
    "- Overall, by incorporating the article length as a custom feature, the model aims to capture any potential relationships between the length of news articles and their political bias classifications, thereby enhancing its ability to accurately classify articles based on their content and length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f781c-017c-408d-89a3-14ebfe15ea52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
